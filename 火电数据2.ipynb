{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oorange-ocean/thermx-data/blob/main/%E7%81%AB%E7%94%B5%E6%95%B0%E6%8D%AE2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jmDXrRjRPdZ"
      },
      "source": [
        "## 初始化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BqdJGHkAfe36"
      },
      "outputs": [],
      "source": [
        "# @title 导入必要的库和设置环境\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 定义文件路径\n",
        "DATA_PATH = '/content/drive/My Drive/data_converted.tsv'\n",
        "SAVE_PATH = '/content/drive/My Drive/cleaned_results.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXVs6DZNlaiG"
      },
      "outputs": [],
      "source": [
        "# @title 加载数据\n",
        "df = pd.read_csv(DATA_PATH, sep='\\t')\n",
        "print(\"数据预览：\", df.head())\n",
        "print(\"\\n数据信息：\", df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "emqEOsOylnoa"
      },
      "outputs": [],
      "source": [
        "#@title 安装中文字体\n",
        "!wget -O simhei.ttf \"https://github.com/StellarCN/scp_zh/raw/master/fonts/SimHei.ttf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file simhei.ttf"
      ],
      "metadata": {
        "id": "fqOnbfsdjpSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn6fTIL1RUn6"
      },
      "source": [
        "## 异常值检测与清理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObgNQjzk69hK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from scipy.stats import skew\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "matplotlib.font_manager.fontManager.addfont('simhei.ttf')\n",
        "matplotlib.rc('font', family='SimHei')\n",
        "\n",
        "def analyze_distribution(df, column_name, bins=50, low_value_threshold=5.0):\n",
        "    \"\"\"分析数据分布并推荐异常值检测方法\"\"\"\n",
        "    data = df[column_name].dropna()\n",
        "    if len(data) == 0:\n",
        "        print(f\"列 '{column_name}' 无有效数据（全为 NaN），跳过分析\")\n",
        "        return None\n",
        "\n",
        "    hist, bin_edges = np.histogram(data, bins=bins, density=True)\n",
        "    peaks, _ = find_peaks(hist, prominence=0.1)\n",
        "    data_skew = skew(data)\n",
        "    low_value_ratio = (data < low_value_threshold).sum() / len(data) * 100\n",
        "\n",
        "    print(f\"列 '{column_name}' 分布分析：\")\n",
        "    print(f\"- 有效数据量：{len(data)}\")\n",
        "    print(f\"- 最小值：{data.min():.2f}, 最大值：{data.max():.2f}\")\n",
        "    print(f\"- 低值区 (<{low_value_threshold}) 比例：{low_value_ratio:.2f}%\")\n",
        "    print(f\"- 偏度：{data_skew:.2f}, 峰值数量：{len(peaks)}\")\n",
        "\n",
        "    if len(peaks) > 1:\n",
        "        method = 'multi_peak'\n",
        "        print(\"推荐方法：多峰处理（分段检测）\")\n",
        "    elif low_value_ratio > 40 and abs(data_skew) > 1:\n",
        "        method = 'low_value_dense'\n",
        "        print(\"推荐方法：低值密集区处理（分段检测）\")\n",
        "    elif abs(data_skew) > 1:\n",
        "        method = 'percentile'\n",
        "        print(\"推荐方法：百分位法（偏态分布）\")\n",
        "    else:\n",
        "        method = 'IQR'\n",
        "        print(\"推荐方法：IQR（均匀或近似正态分布）\")\n",
        "\n",
        "    return method\n",
        "\n",
        "def detect_and_clean_outliers_auto(df, column_name, low_value_threshold=5.0, iqr_multiplier=1.5, percentile_bounds=(1, 99), high_outlier_threshold=5.0, max_outlier_ratio=5.0):\n",
        "    \"\"\"根据分布自动选择方法检测和清理异常值，详略得当\"\"\"\n",
        "    print(f\"\\n检查列 '{column_name}' 的数据状态：\")\n",
        "    print(f\"- 总数据量：{len(df[column_name])}, NaN 数量：{df[column_name].isna().sum()}\")\n",
        "\n",
        "    data = df[column_name].dropna()\n",
        "    if len(data) == 0:\n",
        "        print(f\"列 '{column_name}' 无有效数据（全为 NaN），跳过处理\")\n",
        "        return False, None, None, None\n",
        "\n",
        "    method = analyze_distribution(df, column_name, low_value_threshold=low_value_threshold)\n",
        "    if method is None:\n",
        "        return False, None, None, None\n",
        "\n",
        "    outliers = pd.Series(dtype=float)\n",
        "    low_data = None\n",
        "    high_data = None\n",
        "    split_point = None\n",
        "\n",
        "    if method == 'multi_peak':\n",
        "        # 计算直方图\n",
        "        hist, bin_edges = np.histogram(data, bins=50, density=True)\n",
        "        peaks, _ = find_peaks(hist, prominence=0.1)\n",
        "\n",
        "        if len(peaks) > 1:\n",
        "            # 改进：找到峰值之间的谷值作为分段点\n",
        "            valleys, _ = find_peaks(-hist, prominence=0.05)  # 对直方图取反，找谷值\n",
        "            if len(valleys) > 0:\n",
        "                # 选择最深的谷值（通常在中间的谷值）\n",
        "                valley_idx = valleys[np.argmin(hist[valleys])]\n",
        "                split_point = bin_edges[valley_idx]\n",
        "            else:\n",
        "                # 如果找不到谷值，退回到原方法\n",
        "                split_point = (bin_edges[peaks[0]] + bin_edges[peaks[-1]]) / 2\n",
        "\n",
        "            low_data = data[data <= split_point]\n",
        "            high_data = data[data > split_point]\n",
        "            outliers_idx = []\n",
        "\n",
        "            # 动态调整 iqr_multiplier\n",
        "            for segment, label in [(low_data, '低值段'), (high_data, '高值段')]:\n",
        "                if len(segment) > 0:\n",
        "                    segment_skew = skew(segment)\n",
        "                    # 如果段内偏度较大，增大 iqr_multiplier\n",
        "                    adjusted_iqr_multiplier = iqr_multiplier + 0.5 * abs(segment_skew) if abs(segment_skew) > 0.5 else iqr_multiplier\n",
        "                    print(f\"- {label}：调整后的 IQR 倍数={adjusted_iqr_multiplier:.2f}\")\n",
        "\n",
        "                    Q1 = segment.quantile(0.25)\n",
        "                    Q3 = segment.quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - adjusted_iqr_multiplier * IQR\n",
        "                    upper_bound = Q3 + adjusted_iqr_multiplier * IQR\n",
        "                    segment_outliers = segment[(segment < lower_bound) | (segment > upper_bound)]\n",
        "                    outliers_idx.extend(segment_outliers.index)\n",
        "\n",
        "            outliers = data.loc[outliers_idx]\n",
        "            outlier_ratio = len(outliers) / len(df) * 100\n",
        "\n",
        "            # 如果异常值比例过高，逐步放宽条件\n",
        "            while outlier_ratio > max_outlier_ratio and adjusted_iqr_multiplier < 3.0:\n",
        "                print(f\"异常值占比 {outlier_ratio:.2f}% 过高，尝试放宽 IQR 倍数...\")\n",
        "                adjusted_iqr_multiplier += 0.5\n",
        "                outliers_idx = []\n",
        "                for segment, label in [(low_data, '低值段'), (high_data, '高值段')]:\n",
        "                    if len(segment) > 0:\n",
        "                        Q1 = segment.quantile(0.25)\n",
        "                        Q3 = segment.quantile(0.75)\n",
        "                        IQR = Q3 - Q1\n",
        "                        lower_bound = Q1 - adjusted_iqr_multiplier * IQR\n",
        "                        upper_bound = Q3 + adjusted_iqr_multiplier * IQR\n",
        "                        segment_outliers = segment[(segment < lower_bound) | (segment > upper_bound)]\n",
        "                        outliers_idx.extend(segment_outliers.index)\n",
        "                outliers = data.loc[outliers_idx]\n",
        "                outlier_ratio = len(outliers) / len(df) * 100\n",
        "\n",
        "            df.loc[outliers_idx, column_name] = np.nan\n",
        "\n",
        "    elif method == 'low_value_dense':\n",
        "        low_mask = df[column_name] < low_value_threshold\n",
        "        high_data = data[~low_mask.reindex(data.index, fill_value=False)]\n",
        "        if len(high_data) > 0:\n",
        "            Q1 = high_data.quantile(0.25)\n",
        "            Q3 = high_data.quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - iqr_multiplier * IQR\n",
        "            upper_bound = Q3 + iqr_multiplier * IQR\n",
        "            outliers = high_data[(high_data < lower_bound) | (high_data > upper_bound)]\n",
        "            df.loc[outliers.index, column_name] = np.nan\n",
        "\n",
        "    elif method == 'percentile':\n",
        "        lower_bound, upper_bound = np.percentile(data, percentile_bounds)\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "        df.loc[outliers.index, column_name] = np.nan\n",
        "\n",
        "    else:  # IQR\n",
        "        Q1 = data.quantile(0.25)\n",
        "        Q3 = data.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - iqr_multiplier * IQR\n",
        "        upper_bound = Q3 + iqr_multiplier * IQR\n",
        "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
        "        df.loc[outliers.index, column_name] = np.nan\n",
        "\n",
        "    outlier_count = len(outliers)\n",
        "    outlier_ratio = outlier_count / len(df) * 100\n",
        "\n",
        "    if outlier_count == 0:\n",
        "        print(f\"列 '{column_name}' 无异常值，跳过详细日志和绘图\")\n",
        "        return False, None, None, None\n",
        "\n",
        "    print(f\"检测到 {outlier_count} 个异常值，占比 {outlier_ratio:.2f}%\")\n",
        "\n",
        "    if outlier_ratio > high_outlier_threshold:\n",
        "        print(f\"警告：异常值占比超过 {high_outlier_threshold}%，以下为详细信息：\")\n",
        "        if method == 'multi_peak':\n",
        "            print(f\"- 分段点：{split_point:.2f}\")\n",
        "            for segment, label in [(low_data, '低值段'), (high_data, '高值段')]:\n",
        "                if len(segment) > 0:\n",
        "                    print(f\"- {label}：数据量={len(segment)}, 最小值={segment.min():.2f}, 最大值={segment.max():.2f}\")\n",
        "                    print(f\"  Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}, 下限={lower_bound:.2f}, 上限={upper_bound:.2f}\")\n",
        "        elif method == 'low_value_dense':\n",
        "            print(f\"- 非低值部分：数据量={len(high_data)}, 最小值={high_data.min():.2f}, 最大值={high_data.max():.2f}\")\n",
        "            print(f\"  Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}, 下限={lower_bound:.2f}, 上限={upper_bound:.2f}\")\n",
        "        elif method == 'percentile':\n",
        "            print(f\"- 百分位范围：下限={lower_bound:.2f}, 上限={upper_bound:.2f}\")\n",
        "        else:\n",
        "            print(f\"- IQR 参数：Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}, 下限={lower_bound:.2f}, 上限={upper_bound:.2f}\")\n",
        "\n",
        "    print(f\"\\n列 '{column_name}' 的异常值样本：\")\n",
        "    sample_size = min(5, len(outliers) // 2)\n",
        "    print(\"最小值端：\", outliers.nsmallest(sample_size).to_list())\n",
        "    print(\"最大值端：\", outliers.nlargest(sample_size).to_list())\n",
        "    return True, method, low_data, high_data\n",
        "\n",
        "def plot_distribution(df, column_name, method, low_data=None, high_data=None):\n",
        "    \"\"\"绘制箱线图和直方图，支持分段展示\"\"\"\n",
        "    data = df[column_name].dropna()\n",
        "    if len(data) == 0:\n",
        "        print(f\"列 '{column_name}' 无有效数据（全为 NaN），跳过绘图\")\n",
        "        return\n",
        "\n",
        "    if method == 'multi_peak' and low_data is not None and high_data is not None:\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.boxplot(low_data)\n",
        "        plt.title(f\"Boxplot of {column_name} (Low Segment)\")\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.hist(low_data, bins=50, color='blue', alpha=0.7)\n",
        "        plt.title(f\"Histogram of {column_name} (Low Segment)\")\n",
        "\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.boxplot(high_data)\n",
        "        plt.title(f\"Boxplot of {column_name} (High Segment)\")\n",
        "\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.hist(high_data, bins=50, color='green', alpha=0.7)\n",
        "        plt.title(f\"Histogram of {column_name} (High Segment)\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.boxplot(data)\n",
        "        plt.title(f\"Boxplot of {column_name}\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.hist(data, bins=50)\n",
        "        plt.title(f\"Histogram of {column_name}\")\n",
        "        plt.show()\n",
        "\n",
        "# 处理所有数值列\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    print(f\"\\n=== 处理列 '{col}' ===\")\n",
        "    has_outliers, method, low_data, high_data = detect_and_clean_outliers_auto(\n",
        "        df,\n",
        "        col,\n",
        "        low_value_threshold=5.0,\n",
        "        iqr_multiplier=1.5,\n",
        "        percentile_bounds=(1, 99),\n",
        "        high_outlier_threshold=5.0,\n",
        "        max_outlier_ratio=3.0       # 新增参数：目标异常值比例上限\n",
        "    )\n",
        "    if has_outliers:\n",
        "        plot_distribution(df, col, method, low_data, high_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "301QsX3nlr37"
      },
      "outputs": [],
      "source": [
        "# @title 保存异常清理数据\n",
        "df.to_csv(SAVE_PATH, index=False)\n",
        "print(f\"处理后的数据已保存至：{SAVE_PATH}\")\n",
        "print(\"\\n最终数据预览：\", df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EOKd8flkCWRl"
      },
      "outputs": [],
      "source": [
        "# @title 稳态运行区间提取\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter  # 用于数据平滑\n",
        "\n",
        "# 确保中文字体已加载（基于你之前的代码）\n",
        "plt.rcParams['font.family'] = 'SimHei'\n",
        "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
        "\n",
        "# 定义稳态检测函数\n",
        "def detect_steady_state(df, column_name='机组负荷', window_size=30, std_threshold=5.0, min_duration=10):\n",
        "    data = df[column_name].dropna()\n",
        "    if len(data) == 0:\n",
        "        print(f\"列 '{column_name}' 无有效数据，跳过稳态检测\")\n",
        "        return []\n",
        "\n",
        "    smoothed_data = savgol_filter(data, window_length=window_size, polyorder=2)\n",
        "    rolling_std = data.rolling(window=window_size, center=True).std()\n",
        "    is_steady = rolling_std < std_threshold\n",
        "\n",
        "    steady_intervals = []\n",
        "    start_idx = None\n",
        "\n",
        "    for i in range(len(is_steady)):\n",
        "        if is_steady.iloc[i] and start_idx is None:\n",
        "            start_idx = i\n",
        "        elif not is_steady.iloc[i] and start_idx is not None:\n",
        "            end_idx = i - 1\n",
        "            if (end_idx - start_idx + 1) >= min_duration:\n",
        "                steady_intervals.append((start_idx, end_idx))\n",
        "            start_idx = None\n",
        "\n",
        "    if start_idx is not None and (len(data) - start_idx) >= min_duration:\n",
        "        steady_intervals.append((start_idx, len(data) - 1))\n",
        "\n",
        "    # 定量评估\n",
        "    total_length = len(data)\n",
        "    steady_length = sum(end - start + 1 for start, end in steady_intervals)\n",
        "    coverage = steady_length / total_length * 100\n",
        "    stability = [data.iloc[start:end+1].std() for start, end in steady_intervals]\n",
        "\n",
        "    print(f\"检测到 {len(steady_intervals)} 个稳态运行区间\")\n",
        "    print(f\"稳态覆盖率: {coverage:.2f}%\")\n",
        "    print(f\"各稳态区间标准差: {[f'{s:.2f}' for s in stability]}\")\n",
        "    for idx, (start, end) in enumerate(steady_intervals):\n",
        "        start_time = df['时间'].iloc[start]\n",
        "        end_time = df['时间'].iloc[end]\n",
        "        avg_load = data.iloc[start:end+1].mean()\n",
        "        print(f\"区间 {idx+1}: {start_time} 至 {end_time}, 平均负荷: {avg_load:.2f}\")\n",
        "\n",
        "    return steady_intervals\n",
        "\n",
        "\n",
        "# 绘制稳态区间图\n",
        "def plot_steady_state(df, column_name='机组负荷', steady_intervals=None):\n",
        "    \"\"\"\n",
        "    绘制目标列的时间序列图并标注稳态区间\n",
        "    \"\"\"\n",
        "    data = df[column_name].dropna()\n",
        "    time = pd.to_datetime(df['时间'])\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.plot(time, data, label=f'{column_name} (原始数据)', alpha=0.5)\n",
        "\n",
        "    if steady_intervals:\n",
        "        for start, end in steady_intervals:\n",
        "            plt.axvspan(time.iloc[start], time.iloc[end], color='green', alpha=0.3, label='稳态区间' if start == steady_intervals[0][0] else \"\")\n",
        "\n",
        "    plt.title(f'{column_name} 时间序列与稳态区间')\n",
        "    plt.xlabel('时间')\n",
        "    plt.ylabel(column_name)\n",
        "    plt.legend()\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 执行稳态检测\n",
        "steady_intervals = detect_steady_state(\n",
        "    df,\n",
        "    column_name='机组负荷',      # 使用机组负荷作为稳态判断依据\n",
        "    window_size=30,             # 滑动窗口大小（可根据数据频率调整）\n",
        "    std_threshold=5.0,          # 标准差阈值（可根据实际情况调整）\n",
        "    min_duration=10             # 最小稳态区间长度（数据点数）\n",
        ")\n",
        "\n",
        "# 绘制结果\n",
        "plot_steady_state(df, '机组负荷', steady_intervals)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ltzy4y-XSaFH"
      },
      "outputs": [],
      "source": [
        "# @title 定量评估稳态区间提取（基于固定阈值的对比方法）\n",
        "def detect_steady_state_threshold(df, column_name='机组负荷', threshold=5.0, min_duration=10):\n",
        "    data = df[column_name].dropna()\n",
        "    diff = data.diff().abs()  # 计算相邻点差值\n",
        "    is_steady = diff < threshold\n",
        "\n",
        "    steady_intervals = []\n",
        "    start_idx = None\n",
        "\n",
        "    for i in range(len(is_steady)):\n",
        "        if is_steady.iloc[i] and start_idx is None:\n",
        "            start_idx = i\n",
        "        elif not is_steady.iloc[i] and start_idx is not None:\n",
        "            end_idx = i - 1\n",
        "            if (end_idx - start_idx + 1) >= min_duration:\n",
        "                steady_intervals.append((start_idx, end_idx))\n",
        "            start_idx = None\n",
        "\n",
        "    if start_idx is not None and (len(data) - start_idx) >= min_duration:\n",
        "        steady_intervals.append((start_idx, len(data) - 1))\n",
        "\n",
        "    print(f\"[固定阈值法] 检测到 {len(steady_intervals)} 个稳态运行区间\")\n",
        "    return steady_intervals\n",
        "\n",
        "# 对比实验\n",
        "steady_intervals_threshold = detect_steady_state_threshold(df, '机组负荷', threshold=5.0, min_duration=10)\n",
        "plot_steady_state(df, '机组负荷', steady_intervals_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt8QsC13jK6W"
      },
      "outputs": [],
      "source": [
        "# @title 保存稳态区间数据\n",
        "steady_df = pd.DataFrame()\n",
        "for idx, (start, end) in enumerate(steady_intervals, 1):\n",
        "    steady_segment = df.iloc[start:end+1].copy()\n",
        "    steady_segment['稳态区间编号'] = idx  # 从 1 开始递增\n",
        "    steady_df = pd.concat([steady_df, steady_segment])\n",
        "\n",
        "STEADY_SAVE_PATH = '/content/drive/My Drive/steady_state_data.csv'\n",
        "steady_df.to_csv(STEADY_SAVE_PATH, index=False)\n",
        "print(f\"稳态运行区间数据已保存至：{STEADY_SAVE_PATH}\")\n",
        "print(\"\\n稳态数据预览：\", steady_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkn86pnnWWGU"
      },
      "outputs": [],
      "source": [
        "# @title 特征提取（通用版，使用 TS2Vec）\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from scipy.stats import variation\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 定义文件路径\n",
        "STEADY_SAVE_PATH = '/content/drive/My Drive/steady_state_data.csv'\n",
        "FEATURE_SAVE_PATH = '/content/drive/My Drive/ts2vec_features.csv'\n",
        "\n",
        "# 安装依赖\n",
        "!pip install torch numpy pandas scipy tqdm -q\n",
        "\n",
        "# 下载并导入 TS2Vec\n",
        "!git clone https://github.com/zhihanyue/ts2vec.git -q\n",
        "sys.path.append('/content/ts2vec')\n",
        "from ts2vec import TS2Vec\n",
        "\n",
        "# 加载数据\n",
        "print(\"加载稳态数据...\")\n",
        "steady_df = pd.read_csv(STEADY_SAVE_PATH)\n",
        "print(\"稳态数据预览：\", steady_df.head())\n",
        "\n",
        "# 动态计算稳态区间的数量\n",
        "num_steady_intervals = steady_df['稳态区间编号'].nunique()\n",
        "print(f\"\\n动态计算的稳态区间数量：{num_steady_intervals}\")\n",
        "\n",
        "# 验证稳态区间编号\n",
        "print(\"\\n检查 '稳态区间编号' 的分布：\")\n",
        "print(\"唯一值数量：\", steady_df['稳态区间编号'].nunique())\n",
        "print(\"最小值：\", steady_df['稳态区间编号'].min())\n",
        "print(\"最大值：\", steady_df['稳态区间编号'].max())\n",
        "print(\"前 10 个唯一值：\", sorted(steady_df['稳态区间编号'].unique())[:10])\n",
        "\n",
        "# 如果编号不连续或最大值异常，重新生成连续编号\n",
        "# 期望编号从 1 到 num_steady_intervals\n",
        "if steady_df['稳态区间编号'].min() < 1 or steady_df['稳态区间编号'].max() != num_steady_intervals:\n",
        "    print(\"检测到 '稳态区间编号' 异常，重新生成连续编号...\")\n",
        "    grouped = steady_df.groupby('稳态区间编号')\n",
        "    steady_df['稳态区间编号'] = steady_df['稳态区间编号'].map(\n",
        "        {old_id: new_id for new_id, (old_id, _) in enumerate(grouped, 1)}\n",
        "    )\n",
        "    print(\"修复后 '稳态区间编号' 的分布：\")\n",
        "    print(\"唯一值数量：\", steady_df['稳态区间编号'].nunique())\n",
        "    print(\"最小值：\", steady_df['稳态区间编号'].min())\n",
        "    print(\"最大值：\", steady_df['稳态区间编号'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXJKA8wdesWt"
      },
      "outputs": [],
      "source": [
        "# @title 特征重要性分析与降维\n",
        "numeric_cols = steady_df.select_dtypes(include=[np.number]).columns.drop('稳态区间编号')\n",
        "print(f\"共有 {len(numeric_cols)} 个数值列\")\n",
        "\n",
        "# 计算变异系数\n",
        "def calculate_cv(df, cols):\n",
        "    cv_dict = {}\n",
        "    for col in tqdm(cols, desc=\"计算变异系数\"):\n",
        "        data = df[col].dropna()\n",
        "        if len(data) > 0:\n",
        "            cv_dict[col] = variation(data)\n",
        "    return cv_dict\n",
        "\n",
        "cv_dict = calculate_cv(steady_df, numeric_cols)\n",
        "cv_df = pd.DataFrame(list(cv_dict.items()), columns=['特征', '变异系数']).sort_values('变异系数', ascending=False)\n",
        "\n",
        "# 计算互信息（以‘机组负荷’为目标）\n",
        "def calculate_mutual_info(df, target_col, feature_cols):\n",
        "    X = df[feature_cols].fillna(0)\n",
        "    y = df[target_col].fillna(0)\n",
        "    mi_scores = mutual_info_regression(X, y)\n",
        "    return pd.DataFrame({'特征': feature_cols, '互信息': mi_scores}).sort_values('互信息', ascending=False)\n",
        "\n",
        "target_col = '机组负荷'\n",
        "mi_df = calculate_mutual_info(steady_df, target_col, numeric_cols.drop(target_col))\n",
        "\n",
        "# 计算灰色关联度\n",
        "def calculate_grey_relation(df, target_col, feature_cols):\n",
        "    X = df[feature_cols].fillna(0)\n",
        "    y = df[target_col].fillna(0)\n",
        "    X_norm = (X - X.min()) / (X.max() - X.min())\n",
        "    y_norm = (y - y.min()) / (y.max() - y.min())\n",
        "    diff = np.abs(X_norm.values - y_norm.values[:, None])\n",
        "    rho = 0.5\n",
        "    min_diff = diff.min()\n",
        "    max_diff = diff.max()\n",
        "    grey_coeff = (min_diff + rho * max_diff) / (diff + rho * max_diff)\n",
        "    grey_relation = grey_coeff.mean(axis=0)\n",
        "    return pd.DataFrame({'特征': feature_cols, '灰色关联度': grey_relation}).sort_values('灰色关联度', ascending=False)\n",
        "\n",
        "grey_df = calculate_grey_relation(steady_df, target_col, numeric_cols.drop(target_col))\n",
        "\n",
        "# 必须包含的重点变量\n",
        "must_include = ['主汽压力', '再热汽压力']  # 如果还有其他变量，可以加在这里\n",
        "\n",
        "# 选择关键特征（Top 8，但确保包含 must_include）\n",
        "top_cv = cv_df['特征'].head(4).tolist()\n",
        "top_mi = mi_df['特征'].head(4).tolist()\n",
        "top_grey = grey_df['特征'].head(4).tolist()\n",
        "\n",
        "# 合并特征并确保 must_include 被包含\n",
        "feature_cols = list(set(top_cv + top_mi + top_grey + must_include))\n",
        "print(f\"选择的 {len(feature_cols)} 个关键特征（含必须变量 {must_include}）：\", feature_cols)\n",
        "\n",
        "# @title 数据准备（降采样）\n",
        "def prepare_ts2vec_data(df, feature_cols, group_col='稳态区间编号', max_len=500):\n",
        "    grouped = df.groupby(group_col)\n",
        "    data_list = []\n",
        "\n",
        "    for _, group in tqdm(grouped, desc=\"准备 TS2Vec 数据\"):\n",
        "        features = group[feature_cols].values\n",
        "        if len(features) > max_len:\n",
        "            indices = np.linspace(0, len(features) - 1, max_len).astype(int)\n",
        "            features = features[indices]\n",
        "        data_list.append(features)\n",
        "\n",
        "    padded_data = np.zeros((len(data_list), max_len, len(feature_cols)))\n",
        "    for i, seq in tqdm(enumerate(data_list), total=len(data_list), desc=\"填充数据\"):\n",
        "        padded_data[i, :len(seq), :] = seq\n",
        "\n",
        "    return padded_data\n",
        "\n",
        "ts2vec_data = prepare_ts2vec_data(steady_df, feature_cols, max_len=500)\n",
        "print(\"TS2Vec 输入数据形状：\", ts2vec_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqBo4shtev-f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title 训练 TS2Vec 模型\n",
        "model = TS2Vec(\n",
        "    input_dims=len(feature_cols),\n",
        "    device='cpu',\n",
        "    output_dims=64,\n",
        "    hidden_dims=32,\n",
        "    depth=5,\n",
        "    batch_size=8,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "print(\"开始训练 TS2Vec 模型...\")\n",
        "loss_log = model.fit(\n",
        "    ts2vec_data,\n",
        "    n_epochs=50,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# @title 特征提取\n",
        "print(\"提取特征...\")\n",
        "features = model.encode(ts2vec_data, encoding_window='full_series')\n",
        "print(\"提取的特征形状：\", features.shape)\n",
        "\n",
        "# 构建特征数据框\n",
        "feature_df = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(features.shape[1])])\n",
        "# 手动生成连续编号\n",
        "feature_df['稳态区间编号'] = np.arange(1, len(feature_df) + 1)\n",
        "\n",
        "# 可视化\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(feature_df['feature_0'], feature_df['feature_1'], c=feature_df['稳态区间编号'], cmap='viridis')\n",
        "plt.title('TS2Vec 提取的特征可视化（前两个维度）')\n",
        "plt.xlabel('Feature 0')\n",
        "plt.ylabel('Feature 1')\n",
        "plt.colorbar(label='稳态区间编号')\n",
        "plt.show()\n",
        "\n",
        "# 保存结果\n",
        "print(\"保存特征结果...\")\n",
        "feature_df.to_csv(FEATURE_SAVE_PATH, index=False)\n",
        "print(f\"特征提取结果已保存至：{FEATURE_SAVE_PATH}\")\n",
        "print(\"\\n特征数据预览：\", feature_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYCUIpsPRKCD"
      },
      "source": [
        "## 聚类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUdd0dewhwDC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 文件路径\n",
        "STEADY_SAVE_PATH = '/content/drive/My Drive/steady_state_data.csv'\n",
        "TS2VEC_FEATURE_PATH = '/content/drive/My Drive/ts2vec_features.csv'\n",
        "CLUSTER_SAVE_PATH = '/content/drive/My Drive/clustering_data_ts2vec.csv'\n",
        "\n",
        "# 加载数据\n",
        "print(\"加载稳态数据和 TS2Vec 特征...\")\n",
        "steady_df = pd.read_csv(STEADY_SAVE_PATH)\n",
        "feature_df = pd.read_csv(TS2VEC_FEATURE_PATH)\n",
        "print(\"稳态数据预览：\", steady_df.head())\n",
        "print(\"TS2Vec 特征预览：\", feature_df.head())\n",
        "\n",
        "# 计算原始数据的平均特征\n",
        "print(\"计算原始数据的平均特征...\")\n",
        "steady_grouped = steady_df.groupby('稳态区间编号').agg({\n",
        "    '机组负荷': 'mean',\n",
        "    '汽轮机热耗率q': 'mean'\n",
        "}).reset_index()\n",
        "steady_grouped.rename(columns={\n",
        "    '机组负荷': '平均机组负荷',\n",
        "    '汽轮机热耗率q': '平均热耗率'\n",
        "}, inplace=True)\n",
        "print(\"平均特征预览：\", steady_grouped.head())\n",
        "\n",
        "# 准备 TS2Vec 特征\n",
        "print(\"准备 TS2Vec 特征用于聚类...\")\n",
        "X = feature_df.drop(columns=['稳态区间编号']).values\n",
        "print(\"特征向量形状：\", X.shape)\n",
        "\n",
        "# 标准化特征\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA 降维可视化检查特征区分度\n",
        "print(\"检查 TS2Vec 特征的区分度（PCA 降维）...\")\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=50)\n",
        "plt.title('TS2Vec 特征 PCA 降维可视化')\n",
        "plt.xlabel('主成分 1')\n",
        "plt.ylabel('主成分 2')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print(f\"前两个主成分解释的方差比例：{pca.explained_variance_ratio_.sum():.3f}\")\n",
        "\n",
        "# 方法 1：改进 K-Means 聚类\n",
        "print(\"\\n=== 方法 1：改进 K-Means 聚类 ===\")\n",
        "silhouette_scores = []\n",
        "K = range(2, 10)  # 缩小范围，专注少量簇\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "    score = silhouette_score(X_scaled, labels)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f\"簇数 {k} 的轮廓系数：{score:.3f}\")\n",
        "\n",
        "n_clusters = 5  # 手动指定为 5，基于业务需求而非仅轮廓系数\n",
        "print(f\"手动选择簇数：{n_clusters}\")\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "feature_df['Cluster_KMeans'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# 方法 2：尝试 DBSCAN 聚类\n",
        "print(\"\\n=== 方法 2：DBSCAN 聚类 ===\")\n",
        "dbscan = DBSCAN(eps=2.0, min_samples=5)  # eps 和 min_samples 可调\n",
        "feature_df['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)\n",
        "n_clusters_dbscan = len(set(feature_df['Cluster_DBSCAN'])) - (1 if -1 in feature_df['Cluster_DBSCAN'] else 0)\n",
        "print(f\"DBSCAN 检测到的簇数：{n_clusters_dbscan}\")\n",
        "print(f\"噪声点数量（标签 -1）：{(feature_df['Cluster_DBSCAN'] == -1).sum()}\")\n",
        "\n",
        "# 合并 K-Means 结果到平均特征数据\n",
        "clustered_df = steady_grouped.merge(feature_df[['稳态区间编号', 'Cluster_KMeans', 'Cluster_DBSCAN']],\n",
        "                                    on='稳态区间编号',\n",
        "                                    how='left')\n",
        "clustered_df = clustered_df.dropna(subset=['Cluster_KMeans'])  # 移除未匹配的样本\n",
        "\n",
        "# 生成语义标签（基于 K-Means）\n",
        "def assign_semantic_labels(df, cluster_col='Cluster_KMeans'):\n",
        "    cluster_stats = df.groupby(cluster_col).agg({\n",
        "        '平均机组负荷': 'mean',\n",
        "        '平均热耗率': 'mean'\n",
        "    })\n",
        "    # 动态调整阈值基于数据范围\n",
        "    load_mean, load_std = steady_grouped['平均机组负荷'].mean(), steady_grouped['平均机组负荷'].std()\n",
        "    heat_mean, heat_std = steady_grouped['平均热耗率'].mean(), steady_grouped['平均热耗率'].std()\n",
        "    labels = []\n",
        "    for idx, row in cluster_stats.iterrows():\n",
        "        load = row['平均机组负荷']\n",
        "        heat = row['平均热耗率']\n",
        "        load_label = (\"高负荷\" if load > load_mean + load_std else\n",
        "                      \"低负荷\" if load < load_mean - load_std else \"中负荷\")\n",
        "        heat_label = (\"高效\" if heat < heat_mean - heat_std else\n",
        "                      \"低效\" if heat > heat_mean + heat_std else \"中等效率\")\n",
        "        labels.append(f\"{load_label}-{heat_label}\")\n",
        "    df['Semantic_Label'] = df[cluster_col].map(dict(zip(cluster_stats.index, labels)))\n",
        "    return df\n",
        "\n",
        "clustered_df = assign_semantic_labels(clustered_df, 'Cluster_KMeans')\n",
        "print(\"K-Means 带语义标签的结果：\",\n",
        "      clustered_df[['Cluster_KMeans', 'Semantic_Label', '平均机组负荷', '平均热耗率']].drop_duplicates())\n",
        "\n",
        "# 可视化 K-Means 结果\n",
        "plt.figure(figsize=(10, 6))\n",
        "for cluster_id in clustered_df['Cluster_KMeans'].unique():\n",
        "    cluster_data = clustered_df[clustered_df['Cluster_KMeans'] == cluster_id]\n",
        "    plt.scatter(cluster_data['平均机组负荷'], cluster_data['平均热耗率'],\n",
        "                label=cluster_data['Semantic_Label'].iloc[0], s=100)\n",
        "plt.title(f'K-Means 聚类结果 (基于 TS2Vec 特征, 簇数: {n_clusters})')\n",
        "plt.xlabel('平均机组负荷')\n",
        "plt.ylabel('平均热耗率')\n",
        "plt.legend(title='语义标签')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 可视化 DBSCAN 结果\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(clustered_df['平均机组负荷'], clustered_df['平均热耗率'],\n",
        "            c=clustered_df['Cluster_DBSCAN'], cmap='tab10', s=100)\n",
        "plt.title('DBSCAN 聚类结果 (基于 TS2Vec 特征)')\n",
        "plt.xlabel('平均机组负荷')\n",
        "plt.ylabel('平均热耗率')\n",
        "plt.colorbar(label='聚类标签 (-1 为噪声)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 保存结果\n",
        "clustered_df.to_csv(CLUSTER_SAVE_PATH, index=False)\n",
        "print(f\"聚类结果已保存至：{CLUSTER_SAVE_PATH}\")\n",
        "\n",
        "# 输出统计信息\n",
        "print(\"\\nK-Means 每个聚类的稳态区间数量：\")\n",
        "print(clustered_df['Cluster_KMeans'].value_counts())\n",
        "print(\"\\nK-Means 每个聚类的特征统计：\")\n",
        "print(clustered_df.groupby('Cluster_KMeans').agg({\n",
        "    '平均机组负荷': ['mean', 'std', 'min', 'max'],\n",
        "    '平均热耗率': ['mean', 'std', 'min', 'max']\n",
        "}))\n",
        "print(\"\\nDBSCAN 每个聚类的稳态区间数量：\")\n",
        "print(clustered_df['Cluster_DBSCAN'].value_counts())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOeSNpHXWt/RpucYenAEn73",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}